\documentclass{article}
%\VignetteIndexEntry{Principal Component of Explained Variance}
%\VignetteEngine{knitr::latex}
\usepackage{amsmath,amsthm,amscd,amsfonts,amssymb}

\newcommand{\Var}{\mathrm{Var}}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\newtheorem{thm}{Theorem}[section]

\title{Principal Component of Explained Variance}
\author{Maxime Turgeon}

\begin{document}
\maketitle

\section{General theoretical framework}

We consider the following setting: let $Y$ be a multivariate phenotype of dimension $p$ (e.g. methylation values at $p$ CpG sites, or brain imaging measured at $p$ locations in the brain), let $X$ be a $q$-dimensional vector of covariates of interest (e.g.: smoking, cell type or SNPs) and let $C$ be a $r$-dimensional vector of confounders. We assume that the relationship
between $Y$ and $X$ can be represented via a linear model
$$Y = BX + \Gamma C + E,$$
where $B$ and $\Gamma$ are $p \times q$ and $p \times r$ matrices of regression coefficients for the covariates of interest and confounders, respectively, and $E \sim N (0, \sigma)$ is a vector of residual errors. This model assumption allows us to decompose the total variance of $Y$ as follows:
\begin{align*}
\Var(Y) &= \Var(BX) + \Var(\Gamma C) + \Var(E)\\
  &= B\Var(X)B^T + \Gamma\Var(C)\Gamma^T + \Sigma\\
  &= V_M + V_C + V_R,
\end{align*}
where $V_M = B\Var(X)B^T$ is the model component and $V_R = \Sigma$ is the residual component.
PCEV seeks a linear combination of outcomes, $w^T Y$ , which maximises the ratio $h^2 (w)$ of
variance being explained by the covariates $X$, such that:
\begin{align*}
h^2 (w) &= \frac{\Var(w^T BX)}{\Var(w^T E)}\\
  &= \frac{w^T V_M w}{w^T V_R w}.
\end{align*}
PCEV thus seeks the vector $w$ which maximises the criterion $h^2 (w)$. It follows from a Lagrange multiplier-type argument that $w$ is the solution to the generalised eigenvector problem 
$$V_M w = \lambda V_R w,$$ 
and therefore standard linear algebraic results can be used to get a closed form for $w_\mathrm{PCEV} := \argmax_w h^2 (w)$.

Although there are some similarities between PCA and PCEV since both methods seek a
linear combination of outcomes optimizing a given criterion, we recall that principal compo-
nent analysis (PCA) reduces the dimension of $Y$ by looking for a linear combination of its
components with maximal variance, regardless of the covariates $X$. Furthermore, an impor-
tant difference between PCA and PCEV is in the number of components one can extract.
While the number of components to select in PCA is usually left to the user, and the maximal
number of extracted components is bounded above by $p$, the maximal number of components
that can be extracted for PCEV is bounded above by $q$, the number of covariates. Therefore,
if we are only interested in one covariate, only one PCEV can be extracted; this follows from
considering the rank of the matrix $V_R^{âˆ’1} V_M$.

\subsection{High-dimensional response vector}

When $p$ is larger than the sample size, a na\"{i}ve implementation of PCEV will fail. To ensure the uniqueness of the solution to the maximisation process, the invertibility of the residual matrix $V_R$ is required; therefore, an accurate estimation of the matrix $V_R$ would require a large enough sample size, i.e. $p < n$. This limitation has led some authors to propose regularization techniques for the estimation of $w$, similar in spirit to ridge regression and Lasso. We want to stress once again that these methods require parameters that
are computationally expensive to compute. For this reason, we propose a novel alternative,
namely a block approach to the estimation of PCEV. Assume we can partition $Y$ into blocks
(or clusters) in such a way that the number of components in a given block is small enough
(i.e. smaller than $n$). We can then perform PCEV and get a linear combination $\tilde{Y}_j$ of
the traits belonging to the $j$th block, for each block $j = 1, \ldots, b$. We then obtain a new multivariate pseudo-phenotype:
$$\mathbf{\tilde{Y}} = ( \tilde{Y}_1 , \ldots , \tilde{Y}_b ),$$
on which we can again perform PCEV.

Since the result is a linear combination of linear combinations, it is itself a linear combination of the original traits $Y$. Although one might think that this stepwise approach is an ad-hoc extension of the original PCEV approach, it has nonetheless a very appealing and relevant mathematical property, described in the following result:

\begin{thm}
Assume one can partition the outcomes $Y$ into blocks in such a way that blocks are uncorrelated (i.e. outcomes lying in different blocks are uncorrelated). Then the linear combination (PCEV) obtained from the traditional approach and that obtained from
the stepwise block approach described above are equal.
\end{thm}

Another approach to dealing with a high-dimensional response vector is to replace the classical linear regression estimator of $V_R$ by a shrinkage estimator, e.g. the Ledoit-Wolf linear shrinkage estimator. Both of these approaches are implemented in this packages.

\subsection{Association tests}



\section{Data analysis}

<<data>>=
library(pcev)

data(methylation)
data(pheno)
data(position)
@


<<manPlot>>=
# Compute nominal pvalues
fit <- lm(methylation ~ pheno)
pval <- vapply(summary(fit), function(sum) {
  pvalue <- sum$coef[2,4]
  return(pvalue)
}, numeric(1))

# Manhattan plot univariate
plot(position$Pos/1e6, -log10(pval), xlab="Position (Mb)",
     ylab="-log10 pvalue", pch=19, cex=0.5)
abline(h=-log10(8.3*10^-6), lty=2)
@

<<cluster, eval=FALSE>>=
# Break the region into sub-regions
cl <- bumphunter::clusterMaker(chr=position$Chr, 
                               pos=position$Pos, 
                               assumeSorted=TRUE, 
                               maxGap = 500)

# Some blocks are too big... put limit at 30
index <- cl
maxInd <- max(index) + 1

blockLengths <- table(index)
while(sum(blockLengths > 30) > 0) {
  
  for (j in unique(index)) {
    p <- length(index[index == j])
    if (p > 30) {
      q <- floor(p/2); r <- p - q
      index[index == j] <- c(rep_len(maxInd, q), 
                             rep_len(maxInd + 1, r))
      maxInd <- maxInd + 2
    }
  }
  blockLengths <- table(index)
}

cl <- index
index <- cl
counter <- 0
for(j in sort(unique(cl))) {
  counter <- counter + 1
  index[index == j] <- counter
}
@

<<output>>=
data(index)
table(table(index))

pcev_out <- computePCEV(methylation, covariate = pheno,
                        estimation = "block", 
                        inference = "permutation",
                        index = index, nperm=10)
pcev_out
@

We can construct something resembling a Manhattan plot, but where the univariate p-values are replaced by what we call a \emph{variable importance factor} (or VIMP): it is defined as the correlation between a given response variable $Y_j$ and the PCEV (in absolute value). Figure~\ref{fig:manPlotVIP} shows this type of plot for our particular data. We have also identified the BLK gene using a red line:

<<manPlotVIP, fig.align='center', fig.cap="Manhattan-VIMP plot">>=
# Manhattan plot VIMP
BLK_boundaries <- c(11235000, 11385000)
plot(position$Pos/1e6, pcev_out$VIMP, xlab = "Position (Mb)",
     ylab = "Variable Importance", pch = 19, cex = 0.5, 
     ylim = c(0,1))
lines(x = BLK_boundaries/1e6, y = rep_len(0.9,2),
      lwd = 3, col = 'red')
@

As we can see, the VIMP can serve as surrogates for the univariate p-values when it comes to identifying the most important response variables. In our case, it is able to capture the gene of interest.

\section{Session Info}

<<>>=
sessionInfo()
@

\end{document}